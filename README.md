# Portfolio RAG Assistant

Retrieval-Augmented Generation (RAG) toolkit for showcasing a software/data portfolio. The system ingests your local project folder, builds a persistent ChromaDB index with state-of-the-art BGE embeddings, and answers reviewer questions through a DeepSeek LLM. Use it from the terminal or via a polished Dash UI.

## Highlights
- **Deep ingestion coverage:** Handles Markdown, Python, PDF, DOCX, plain text, and Jupyter notebooks (cell-level granularity) with smart metadata tagging.
- **Code-aware chunking:** Separates prose and code so Python files use language-aware splitting while documentation uses larger narrative chunks.
- **Robust retrieval:** Normalized BAAI/bge-m3 embeddings + Maximal Marginal Relevance retriever for diverse, relevant context.
- **Flexible answering:** Query the portfolio from the CLI or Dash app, swapping DeepSeek chat models without redeploying.

## Architecture
1. **Ingestion (src/ingest.py)**
   - Scans portfolio_data/, skipping common cache/build folders.
   - Loads files with dedicated loaders (Docx2txt, NotebookLoader, etc.), normalizes metadata, and applies token-aware chunking.
   - Embeds chunks with BAAI/bge-m3 (normalized vectors) and persists them to a portfolio collection in chroma_db/.
2. **Retrieval + Generation (src/main.py)**
   - Recreates the same BGE embedding pipeline, opens the portfolio collection, and retrieves context (k=3).
   - Streams grounded answers from DeepSeek via LangChain.
3. **Interactive UI (App.py)**
   - Dash + Bootstrap interface mirroring the CLI pipeline.
   - Prompts for the DeepSeek API key at runtime, exposes instructions, and maintains a history panel.

```
portfolio_data/  -->  ingest.py  -->  chroma_db/ (collection: portfolio)
                                           |
                 +-------------------------+------------------------+
                 |                                                  |
             main.py (CLI)                                    app.py (Dash)
```

## Project Structure
```
.
|-- assets/
|   -- custom.css          # Dash styling (fonts, glassmorphism, history panel)
|-- chroma_db/              # Persisted vector store (generated by ingest.py)
|-- portfolio_data/         # Place your portfolio projects here
|-- src/
|   |-- ingest.py           # Advanced loaders, chunking, BGE embeddings, Chroma persistence
|   -- main.py             # CLI RAG assistant (DeepSeek via LangChain)
|-- app.py                  # Dash web app for interactive exploration
|-- requirements.txt        # Python dependencies
-- README.md
```

## Getting Started
1. **Prepare your data:** Clone/unzip the repo and drop all portfolio projects (any depth) inside portfolio_data/.
2. **Install dependencies** (Python 3.10+ recommended):
   ```Bash
   pip install -r requirements.txt
   ```
3. **Ingest the projects** whenever files change:
   ```Bash
   python src/ingest.py
   ```
   This creates/updates chroma_db/ with the portfolio collection.
4. **Choose your interface:**
   - **Terminal assistant**
     ```Bash
     python src/main.py
     ```
     Provide your DeepSeek API key when prompted (or export DEEPSEEK_API_KEY). Ask questions in natural language.
   - **Dash web app**
     ```Bash
     python app.py
     ```
     Open http://127.0.0.1:8050, paste your API key, pick a model, and start querying. The UI highlights configuration tips and keeps the conversation history.

## Configuration & Customization
- **DeepSeek settings:**
  - DEEPSEEK_API_KEY � optional environment variable to skip the prompt.
  - DEEPSEEK_CHAT_MODEL / DEEPSEEK_BASE_URL � override defaults for alternative endpoints or models.
- **Loader support:** Extend LOADER_MAPPING in src/ingest.py to add CSVs, HTML, proprietary formats, or tweak notebook output inclusion.
- **Chunking parameters:** Adjust prose/code chunk sizes and overlaps to balance recall vs. latency.
- **Retriever behaviour:** Modify RETRIEVER_K, RETRIEVER_FETCH_K, or switch search_type if you prefer plain similarity search.
- **Embeddings:** Swap EMBEDDINGS_MODEL for a lighter model if resources are limited, but remember to adjust main.py and pp.py accordingly.

## Sample Prompts
- *"Resume la arquitectura del proyecto de detecci�n de fraude."*
- *"�Qu� m�tricas se reportan en el notebook de churn?"*
- *"Explica los pasos para entrenar el modelo de series temporales."*

## Troubleshooting
- **"Vector store not found"** � run python src/ingest.py so chroma_db/ exists before launching the assistant/app.
- **Dimension mismatch errors** � verify all entry points use the same embedding model (BAAI/bge-m3) and rerun ingestion after changing loaders or embeddings.
- **Dash callback ID warnings** � make sure the app server picked up the latest layout (stop and restart python app.py).

## Roadmap Ideas
- Automate ingestion via pre-commit or CI pipelines.
- Add evaluators that compare retrieved chunks against generated answers.
- Support additional local LLMs (Ollama, GGML) through the LangChain interface.

## License
This project inherits the repository license (see LICENSE).

## Author Notes
Built to demonstrate end-to-end MLOps thinking: data ingestion, semantic search, and UX. Tailor the loaders, prompt, and UI so reviewers experience the narrative you want to showcase.

# Portfolio RAG Assistant

Retrieval-Augmented Generation (RAG) pipeline designed to showcase a data scientist portfolio. The project ingests local GitHub projects, stores semantic embeddings in ChromaDB, and serves answers through a DeepSeek-powered LLM. It offers both a terminal workflow and a polished Dash web app so reviewers can explore your body of work conversationally.

## Why It Matters for Data Science Portfolios
- **Evidence-based storytelling:** Ground every answer in the actual source files of your projects.
- **Multi-format literacy:** Parse Python, Markdown, PDF, and text files without manual curation.
- **Human-in-the-loop transparency:** Interactive UI exposes instructions, configuration, and the full response history.
- **Model flexibility:** Swap between DeepSeek models (chat vs. reasoner) without code changes.

## Architecture
1. **Ingestion (`src/ingest.py`)**
   - Walks `portfolio_data/`, loads supported files, and chunks text via `RecursiveCharacterTextSplitter`.
   - Embeds chunks with `sentence-transformers/all-MiniLM-L6-v2` (Hugging Face) and persists to ChromaDB.
2. **Retrieval + Generation (`src/main.py`)**
   - Loads the persisted vector store, retrieves top-k context, and passes it to DeepSeek through LangChain.
   - Streams answers in the terminal, enforcing grounded responses.
3. **Interactive UI (`app.py`)**
   - Dash + Bootstrap interface with glassmorphism styling, guided instructions, and query history.
   - Accepts the DeepSeek API key securely at runtime -- never stored on disk.

```
portfolio_data/  -->  ingest.py  -->  chroma_db/  -->  main.py / app.py
```

## Project Structure
```
.
|-- assets/
|   `-- custom.css          # Dash styling (fonts, glassmorphism, history panel)
|-- chroma_db/              # Persisted vector store (generated by ingest.py)
|-- portfolio_data/         # Place your portfolio projects here
|-- src/
|   |-- ingest.py           # Document loaders, chunking, embeddings, Chroma persistence
|   `-- main.py             # CLI RAG assistant (DeepSeek via LangChain)
|-- app.py                  # Dash web app for interactive exploration
|-- requirements.txt        # Python dependencies
`-- README.md
```

## Getting Started
1. **Clone or unzip the repo** and ensure your portfolio files live in `portfolio_data/` (subfolders allowed).
2. **Install dependencies** (Python 3.10+ recommended):
   ```bash
   pip install -r requirements.txt
   ```
3. **Ingest your projects** to build the vector store (run once, or whenever portfolio files change):
   ```bash
   python src/ingest.py
   ```
4. **Choose your interface:**
   - **Terminal assistant:**
     ```bash
     python src/main.py
     ```
     Enter your DeepSeek API key when prompted, then ask natural-language questions.
   - **Dash web app:**
     ```bash
     python app.py
     ```
     Visit `http://127.0.0.1:8050`, paste your API key, pick a model, and start exploring. The app lists usage instructions and keeps a query history.

## Configuration & Customization
- **DeepSeek credentials:** Provided at runtime. Optionally set `DEEPSEEK_API_KEY` or `DEEPSEEK_CHAT_MODEL` as environment variables to skip prompts or change defaults.
- **Supported file types:** Extend `LOADER_MAPPING` in `src/ingest.py` to add CSVs, notebooks, or custom loaders.
- **Embedding model:** Swap `EMBEDDINGS_MODEL_NAME` with any Hugging Face sentence-transformer suitable for your hardware.
- **Retriever behavior:** Adjust `search_kwargs` in `src/main.py` (and `app.py`) to experiment with different `k` values or similarity metrics.

## Demo Queries
- *"Resume the model training pipeline used in the churn prediction project."*
- *"Which evaluation metrics are reported in the fraud detection notebook?"*
- *"What improvements were suggested for the time-series forecasting project?"*

## Roadmap Ideas
- Add automated ingestion triggers (pre-commit hook or GitHub Action).
- Introduce document ranking heuristics (e.g., metadata filters per repository).
- Plug in evaluation dashboards that summarize retrieved context versus generated answer.
- Support additional LLM providers (Ollama, Anthropic, etc.) via the same LangChain interface.

## License
This project inherits the repository license (see `LICENSE`).

## Author Notes
Built as part of a personal data science portfolio: demonstrate end-to-end MLOps thinking, from data ingestion to UX. Adapt freely and tailor the dataset to the narrative you want reviewers to experience.
